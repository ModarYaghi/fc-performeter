{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No password for tt_pt_FM_v04.11.xlsx\n"
     ]
    }
   ],
   "source": [
    "from src.all_in_one import *\n",
    "import os\n",
    "\n",
    "# Set the pandas option\n",
    "pd.set_option(\"future.no_silent_downcasting\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528454c9601986a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = path_manager.processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5603c8e8edbaa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all rid across all datasets\n",
    "rid = pd.DataFrame()  # An empty dataframe\n",
    "if os.path.exists(processed_data_path) and os.path.isdir(processed_data_path):\n",
    "    for file_name in os.listdir(processed_data_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(processed_data_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df_rid = df[\"rid\"]\n",
    "            rid = pd.concat([rid, df_rid])\n",
    "rid = rid.drop_duplicates(keep=\"first\")\n",
    "rid = rid.sort_values(by=\"rid\", ascending=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9e576ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rid\n",
    "# rid.to_clipboard(index=False)\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d7d1d84b927cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's better to consolidate all rid, firstnames, lastnames, sex, and age across all datasets first.\n",
    "# because some of the referral id don't have these values from the screening.\n",
    "# So, i needed to collect them from other services.\n",
    "\n",
    "sub_id = pd.read_excel(\n",
    "    r\"notebooks/consolidate_datasets/sub_id.xlsx\"\n",
    ")  # It seems that I forget how I created \"sub_id.xlsx\"!\n",
    "merged_df = sub_id.copy()  # Initialize merged_df to avoid referencing before assignment\n",
    "\n",
    "for filename in os.listdir(processed_data_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(processed_data_path, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if the needed columns exist in this dataset\n",
    "        columns_needed = [\"rid\", \"firstname\", \"lastname\", \"sex\", \"age\"]\n",
    "        common_columns = [col for col in columns_needed if col in data.columns]\n",
    "\n",
    "        # Ensure 'rid' is present before merging\n",
    "        if \"rid\" in common_columns:\n",
    "            merged_df = merged_df.merge(\n",
    "                data[common_columns],\n",
    "                on=\"rid\",\n",
    "                how=\"left\",\n",
    "                suffixes=(\"\", f\"_from_{filename}\"),\n",
    "            )\n",
    "\n",
    "# Optional: Consolidate columns, for example, firstname from different files\n",
    "for col in [\"firstname\", \"lastname\", \"sex\", \"age\"]:\n",
    "    columns_to_combine = [c for c in merged_df.columns if c.startswith(col)]\n",
    "    merged_df[col] = (\n",
    "        merged_df[columns_to_combine].bfill(axis=1).iloc[:, 0].infer_objects()\n",
    "    )\n",
    "    merged_df.drop(\n",
    "        columns=columns_to_combine[1:], inplace=True\n",
    "    )  # Drop extra columns after combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591150837c0b40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.drop_duplicates(subset=\"rid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dbadf77b48fc4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "# df.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540564c72e95584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screening Dataset\n",
    "scr_file = path_manager.get_data_file(Category.PS, PSFile.SCR)\n",
    "scr_dataset = Dataset(config_file, scr_file.sheet)\n",
    "scr = (\n",
    "    get_df(scr_file.path, scr_file.sheet, config_file)\n",
    "    .sort_values(by=\"sc_s1\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "370ac39a953bf866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Screening basic session separated form pei_pre_as and re-screening\n",
    "# columns_to_drop = ['scspi','nat', 'need_mhpss', 'need_trw', 'need_out_ref', 'need_tmh', 'need_pei', 'note','pei_pre_as', 'sc_re']\n",
    "columns_to_drop = [\"note\", \"pei_pre_as\", \"sc_re\"]\n",
    "scr_0 = (\n",
    "    scr.drop(columns=columns_to_drop).dropna(subset=[\"sc_s1\"]).sort_values(by=\"sc_s1\")\n",
    ")  # Screening basic session. Records = 705\n",
    "scr_0 = scr_0.drop_duplicates(\n",
    "    subset=[\"rid\"], keep=\"first\"\n",
    ")  # There are 6 rid duplicated.\n",
    "re_scr = scr[[\"rid\", \"sc_re\"]].dropna()  # Screening re. Records = 41\n",
    "pei_pre_as = scr[[\"rid\", \"pei_pre_as\"]].dropna()  # PEI Assessment. Records = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475c35d7cf8581f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pei_pre_as.shape\n",
    "# re_scr.shape\n",
    "# scr_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cab7b831e0448",
   "metadata": {},
   "source": [
    "Note: You need to rearrange the two columns to be in their positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5e1cf2ebf589285",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_dataset_scr = pd.merge(rid, scr_0, on=\"rid\", how=\"left\")\n",
    "consolidated_dataset_scr = pd.merge(\n",
    "    consolidated_dataset_scr, re_scr, on=\"rid\", how=\"left\"\n",
    ")\n",
    "consolidated_dataset_scr = pd.merge(\n",
    "    consolidated_dataset_scr, pei_pre_as, on=\"rid\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf076cdd9bbe88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidated_dataset_scr.to_csv('consolidated_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d19b018a736cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(r'notebooks/consolidate_datasets/jun24/consolidated_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
